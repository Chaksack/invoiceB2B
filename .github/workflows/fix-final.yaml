name: InvoiceB2B CI/CD Pipeline

# Workflow for deploying infrastructure using Terraform and deploying applications to ECS
on:
  push:
    branches: [ main, staging ]
  pull_request:
    branches: [ main, staging ]
    types: [opened, synchronize, reopened]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        type: choice
        options:
          - dev
          - staging
          - prod
        default: 'staging'
      destroy_infrastructure:
        description: 'Destroy infrastructure instead of deploying'
        required: false
        type: boolean
        default: false
      confirmation:
        description: 'Type "destroy" to confirm destruction of infrastructure (only needed when destroy_infrastructure is true)'
        required: false
        type: string

permissions:
  contents: read
  pull-requests: write # For SonarQube comments or PR labels
  issues: write     # For manual approval issues

env:
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  TERRAFORM_VERSION: 1.7.0
  # Default environment is staging, can be overridden by workflow_dispatch input
  ENVIRONMENT: ${{ github.event.inputs.environment || 'staging' }}
  COMPOSE_PROJECT_NAME: ${{ secrets.ECS_PROJECT_NAME || 'invoiceb2b' }}
  TERRAFORM_STATE_BUCKET: "invoiceapp-terraform-state"
  TERRAFORM_LOCK_TABLE: "invoice-terraform-locks"

jobs:
  # Reusable job for setting up Terraform
  setup-terraform:
    name: 'Setup Terraform'
    runs-on: ubuntu-latest
    outputs:
      terraform_initialized: ${{ steps.terraform_init.outputs.initialized }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Configure AWS credentials
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Cache Terraform to speed up workflow
      - name: Cache Terraform
        uses: actions/cache@v4
        with:
          path: ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-terraform-${{ hashFiles('**/.terraform.lock.hcl') }}
          restore-keys: |
            ${{ runner.os }}-terraform-

      # Setup Terraform with latest stable version
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      # Initialize Terraform with proper backend configuration
      - name: Terraform Init
        id: terraform_init
        run: |
          # Check if S3 bucket exists
          if aws s3 ls s3://${{ env.TERRAFORM_STATE_BUCKET }} 2>&1 | grep -q 'NoSuchBucket'; then
            echo "S3 bucket for Terraform state does not exist. Creating it..."
            # Initialize with local backend first
            terraform init -backend=false

            # Navigate to bootstrap directory
            cd bootstrap

            # Initialize and apply bootstrap configuration
            echo "Initializing bootstrap Terraform configuration..."
            terraform init

            echo "Creating S3 bucket and DynamoDB table for Terraform backend..."
            terraform apply -auto-approve

            # Navigate back to main directory
            cd ..

            # Now initialize with S3 backend
            terraform init -force-copy -backend-config="key=environments/${{ env.ENVIRONMENT }}/terraform.tfstate"
          else
            echo "S3 bucket for Terraform state exists. Proceeding with normal initialization..."
            # Initialize with environment-specific state file
            terraform init -reconfigure -backend-config="key=environments/${{ env.ENVIRONMENT }}/terraform.tfstate"
          fi
          echo "Terraform initialized with environment-specific state file."
          echo "initialized=true" >> $GITHUB_OUTPUT

  # Validation job for destroy confirmation
  validate-destroy-confirmation:
    name: 'Validate Destroy Confirmation'
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.destroy_infrastructure == 'true'
    steps:
      - name: Check confirmation
        if: ${{ github.event.inputs.confirmation != 'destroy' }}
        run: |
          echo "Error: You must type 'destroy' to confirm infrastructure destruction."
          exit 1

  # Vulnerability scanning job that runs before infrastructure deployment
  vulnerability-scan:
    name: 'Scan for Vulnerabilities'
    runs-on: ubuntu-latest
    # Skip vulnerability scanning when destroying infrastructure
    if: github.event_name != 'workflow_dispatch' || github.event.inputs.destroy_infrastructure != 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Required for SonarQube for full analysis history

      # Setup Node.js for npm audit
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '20'

      # Setup Go for Go code scanning
      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.22' # Using the latest stable Go version

      # Cache Go modules
      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-

      # Scan frontend dependencies
      - name: Scan Frontend Dependencies
        run: |
          cd frontend
          echo "Running npm audit on frontend dependencies..."
          npm audit --production --audit-level=high || echo "Frontend has vulnerabilities that need to be addressed"

      # Scan client dependencies (Golang)
      - name: Scan Internal Dependencies
        run: |
          cd client || cd .
          echo "Running go mod verify for client dependencies..."
          go mod verify || echo "Client has dependency issues that need to be addressed"

      # Setup Terraform for tfsec
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      # Install tfsec for Terraform security scanning
      - name: Install tfsec
        run: |
          curl -s https://raw.githubusercontent.com/aquasecurity/tfsec/master/scripts/install_linux.sh | bash

      # Scan Terraform code
      - name: Scan Terraform Code
        run: |
          echo "Running tfsec on Terraform code..."
          tfsec . --no-color || echo "Terraform has security issues that need to be addressed"

      # SonarQube Scan
      - name: SonarQube Scan
        uses: sonarsource/sonarqube-scan-action@v2.0.0
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        continue-on-error: true

      # Generate vulnerability report
      - name: Generate Vulnerability Report
        run: |
          echo "## Dependency Vulnerability Scan Results" >> $GITHUB_STEP_SUMMARY
          echo "### Frontend Dependencies" >> $GITHUB_STEP_SUMMARY
          cd frontend && npm audit --json | jq -r '.advisories | length | "Found \(.) vulnerabilities"' >> $GITHUB_STEP_SUMMARY || echo "Error running npm audit for frontend" >> $GITHUB_STEP_SUMMARY

          echo "### Client Dependencies (Golang)" >> $GITHUB_STEP_SUMMARY
          cd ../client && go mod verify >> $GITHUB_STEP_SUMMARY 2>&1 || echo "Error verifying Go modules for client" >> $GITHUB_STEP_SUMMARY || cd .. && echo "Client not found or Go modules issue" >> $GITHUB_STEP_SUMMARY

          echo "### Terraform Security Issues" >> $GITHUB_STEP_SUMMARY
          cd .. && tfsec . --no-color --format json | jq -r '.results | length | "Found \(.) security issues"' >> $GITHUB_STEP_SUMMARY || echo "Error running tfsec" >> $GITHUB_STEP_SUMMARY

  # Conditional job that runs when destroy_infrastructure is true
  terraform-destroy:
    name: 'Destroy Infrastructure'
    runs-on: ubuntu-latest
    needs: [validate-destroy-confirmation, setup-terraform]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.destroy_infrastructure == 'true'
    permissions:
      issues: write
      contents: read
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Configure AWS credentials
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Setup Terraform with latest stable version
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      # Initialize Terraform with proper backend configuration
      - name: Terraform Init
        run: |
          # Initialize with environment-specific state file
          terraform init -reconfigure -backend-config="key=environments/${{ env.ENVIRONMENT }}/terraform.tfstate"
          echo "Terraform initialized with environment-specific state file."

      # Create destroy plan (excluding S3 bucket with prevent_destroy=true)
      - name: Terraform Plan Destroy
        run: |
          # Create destroy plan excluding the S3 bucket that has prevent_destroy=true
          terraform state list | grep -q "aws_s3_bucket.terraform_state" && \
            echo "Excluding S3 bucket with prevent_destroy=true from destroy plan" && \
            terraform plan -destroy -var="environment=${{ env.ENVIRONMENT }}" -out=tfdestroyplan -target=$(terraform state list | grep -v "aws_s3_bucket.terraform_state") || \
            terraform plan -destroy -var="environment=${{ env.ENVIRONMENT }}" -out=tfdestroyplan

      # Add a manual approval step
      - name: Manual Approval
        uses: trstringer/manual-approval@v1
        with:
          secret: ${{ github.TOKEN }}
          approvers: ${{ github.actor }}
          minimum-approvals: 1
          issue-title: "Approve Infrastructure Destruction"
          issue-body: "Please approve the destruction of the infrastructure by adding a comment with 'approve'."
          exclude-workflow-initiator-as-approver: false
          timeout-minutes: 10

      # Apply destroy plan
      - name: Terraform Destroy
        run: |
          echo "Applying destroy plan..."
          terraform apply -auto-approve tfdestroyplan

          # Check if destroy was successful
          if [ $? -eq 0 ]; then
            echo "Terraform destroy completed successfully."
          else
            echo "Terraform destroy encountered issues. This might be due to resources with prevent_destroy=true or dependencies."
            echo "Attempting targeted destroy for remaining resources..."

            # List remaining resources
            REMAINING_RESOURCES=$(terraform state list || echo "")

            if [ -n "$REMAINING_RESOURCES" ]; then
              echo "Remaining resources in state:"
              echo "$REMAINING_RESOURCES"

              # Try to destroy each resource individually, skipping the S3 bucket with prevent_destroy=true
              for resource in $REMAINING_RESOURCES; do
                if [[ "$resource" != "aws_s3_bucket.terraform_state" ]]; then
                  echo "Attempting to destroy: $resource"
                  terraform destroy -auto-approve -target=$resource -var="environment=${{ env.ENVIRONMENT }}" || echo "Could not destroy $resource, continuing..."
                else
                  echo "Skipping S3 bucket with prevent_destroy=true: $resource"
                fi
              done
            else
              echo "No resources found in state. Destroy may have been successful despite errors."
            fi
          fi

      # Notify destruction status
      - name: Notify Destruction Status
        if: always()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "${{ job.status == 'success' && '✅ Terraform Infrastructure Destroyed Successfully' || '❌ Terraform Destroy Failed' }} for `${{ github.repository }}`",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "${{ job.status == 'success' && '✅ *Terraform Infrastructure Destroyed Successfully*' || '❌ *Terraform Destroy Failed*' }}\nRepository: `${{ github.repository }}`\nEnvironment: `${{ env.ENVIRONMENT }}`"
                  }
                },
                {
                  "type": "context",
                  "elements": [
                    { "type": "mrkdwn", "text": "Triggered by: `${{ github.actor }}`" },
                    { "type": "mrkdwn", "text": "Workflow Run: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Workflow>" }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  terraform:
    name: 'Deploy Infrastructure'
    needs: [vulnerability-scan, setup-terraform]
    runs-on: ubuntu-latest
    if: always() && (github.event_name != 'workflow_dispatch' || github.event.inputs.destroy_infrastructure != 'true')
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}

    # Define outputs that can be used by other jobs
    outputs:
      terraform_success: ${{ steps.terraform_apply.outputs.success }}
      backup_created: ${{ steps.terraform_apply.outputs.backup_created }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Configure AWS credentials
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Setup Terraform with latest stable version
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      # Initialize Terraform with proper backend configuration
      - name: Terraform Init
        run: |
          # Initialize with environment-specific state file
          terraform init -reconfigure -backend-config="key=environments/${{ env.ENVIRONMENT }}/terraform.tfstate"
          echo "Terraform initialized with environment-specific state file."

      # Check formatting
      - name: Terraform Format
        run: terraform fmt -check -recursive
        continue-on-error: true

      # Validate configuration
      - name: Terraform Validate
        run: terraform validate

      # Create execution plan
      - name: Terraform Plan
        run: terraform plan -var="environment=${{ env.ENVIRONMENT }}" -out=tfplan

      # Add a manual approval step for production deployments
      - name: Manual Approval for Production
        if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request' && env.ENVIRONMENT == 'prod'
        uses: trstringer/manual-approval@v1
        with:
          secret: ${{ github.TOKEN }}
          approvers: ${{ github.actor }}
          minimum-approvals: 1
          issue-title: "Approve Production Deployment"
          issue-body: "Please approve the deployment to the production environment by adding a comment with 'approve'."
          exclude-workflow-initiator-as-approver: false
          timeout-minutes: 60

      # Apply changes only on main branch and not in pull requests
      - name: Terraform Apply
        id: terraform_apply
        if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request' || github.ref == 'refs/heads/staging'
        run: |
          # Apply the changes
          if terraform apply -auto-approve tfplan; then
            echo "Terraform apply succeeded"
            echo "success=true" >> $GITHUB_OUTPUT
            echo "backup_created=true" >> $GITHUB_OUTPUT
          else
            echo "Terraform apply failed"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      # Upload state backup as an artifact
      - name: Upload State Backup
        if: steps.terraform_apply.outputs.backup_created == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: terraform-state-backup-${{ github.event.inputs.environment || 'staging' }}
          path: terraform.tfstate
          retention-days: 7
          if-no-files-found: warn

  build-and-push:
    name: 'Build and Push Docker Images'
    needs: terraform
    runs-on: ubuntu-latest
    # Only run on main branch and not in pull requests, and only if terraform was successful
    if: (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging') && github.event_name != 'pull_request' && needs.terraform.outputs.terraform_success == 'true'
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}

    # Define outputs that can be used by other jobs
    outputs:
      ecr_repos_found: ${{ steps.get-ecr-urls.outputs.ecr_repos_found }}
      api_image_uri: ${{ steps.build-image-api.outputs.ecr_image_uri }}
      image_tag: ${{ steps.build-image-api.outputs.image_tag }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Configure AWS credentials
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Set up Docker Buildx for multi-platform builds and caching
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Login to Amazon ECR
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      # Initialize Terraform to get outputs
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      # Initialize Terraform with proper backend configuration
      - name: Terraform Init
        run: |
          # Initialize with environment-specific state file
          terraform init -reconfigure -backend-config="key=environments/${{ env.ENVIRONMENT }}/terraform.tfstate"
          echo "Terraform initialized with environment-specific state file."

      # Get ECR Repository URLs from Terraform outputs
      - name: Get ECR Repository URLs
        id: get-ecr-urls
        run: |
          # Only check for API repository as per requirements
          API_REPO=$(terraform output -raw api_repository_url || echo "")

          # Check if repository was found
          if [ -z "$API_REPO" ]; then
            echo "ECR repository not found in Terraform outputs. Skipping push."
            echo "ecr_repos_found=false" >> $GITHUB_OUTPUT
          else
            echo "ECR repository found. Proceeding with push."
            echo "ecr_repos_found=true" >> $GITHUB_OUTPUT
          fi

      # Cache Docker layers to speed up builds
      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      # Build, tag, and push API image to Amazon ECR using Buildx for efficient caching
      - name: Build and push API image to Amazon ECR
        id: build-image-api
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.login-ecr.outputs.registry }}/${{ secrets.ECR_REPOSITORY_API }}:${{ github.sha }}
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY_API: ${{ secrets.ECR_REPOSITORY_API }}
          IMAGE_TAG: ${{ github.sha }}

      # Set outputs for downstream jobs
      - name: Set outputs
        run: |
          echo "ecr_image_uri=${{ steps.login-ecr.outputs.registry }}/${{ secrets.ECR_REPOSITORY_API }}:${{ github.sha }}" >> $GITHUB_OUTPUT
          echo "image_tag=${{ github.sha }}" >> $GITHUB_OUTPUT
          echo "Successfully pushed ${{ steps.login-ecr.outputs.registry }}/${{ secrets.ECR_REPOSITORY_API }}:${{ github.sha }}"

      # Move cache to prevent it from growing indefinitely
      - name: Move cache
        run: |
          rm -rf /tmp/.buildx-cache || true
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache || true

  docker-image-scan:
    name: 'Scan Docker Images for Vulnerabilities'
    needs: build-and-push
    runs-on: ubuntu-latest
    # Run even if build-and-push job didn't push to ECR (we can still scan local images)
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging') && github.event_name != 'pull_request'
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}

    # Define outputs that can be used by other jobs
    outputs:
      scan_status: ${{ steps.check-vulnerabilities.outputs.scan_status }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Configure AWS credentials for ECR access
      - name: Configure AWS Credentials
        if: needs.build-and-push.outputs.ecr_repos_found == 'true'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Set up Docker to access images from previous job
      - name: Set up Docker
        uses: docker/setup-buildx-action@v3

      # Install Trivy scanner
      - name: Install Trivy
        run: |
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin v0.47.0

      # Prepare images for scanning
      - name: Prepare Images for Scanning
        id: prepare-images
        run: |
          # Only scan API image as per requirements
          if [[ "${{ needs.build-and-push.outputs.ecr_repos_found }}" == "true" ]]; then
            # Use ECR image if available
            API_IMAGE="${{ needs.build-and-push.outputs.api_image_uri }}"

            # Login to ECR and pull image
            echo "Using ECR image for scanning"
            aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $(echo $API_IMAGE | cut -d: -f1)

            if [ ! -z "$API_IMAGE" ]; then
              echo "Pulling API image: $API_IMAGE"
              docker pull $API_IMAGE || echo "Failed to pull API image"
            fi
          else
            # Use locally built image
            echo "Using locally built image for scanning"
            API_IMAGE="api:latest"

            # Build image locally if needed
            echo "Building API image"
            docker build -t $API_IMAGE -f Dockerfile . || echo "Failed to build API image"
          fi

          # Export image name for later steps
          echo "api_image=$API_IMAGE" >> $GITHUB_OUTPUT

      # Scan images and check for vulnerabilities
      - name: Scan Images for Vulnerabilities
        id: check-vulnerabilities
        run: |
          API_IMAGE="${{ steps.prepare-images.outputs.api_image }}"
          SCAN_STATUS="success"

          echo "## Docker Image Vulnerability Scan Results" >> $GITHUB_STEP_SUMMARY

          # Only scan API image as per requirements
          if [ ! -z "$API_IMAGE" ]; then
            echo "Scanning API Docker image for vulnerabilities..."
            echo "### API Image" >> $GITHUB_STEP_SUMMARY
            if trivy image --format json --severity HIGH,CRITICAL --exit-code 1 $API_IMAGE > api_scan.json 2>/dev/null; then
              echo "No HIGH or CRITICAL vulnerabilities found in API image" >> $GITHUB_STEP_SUMMARY
            else
              SCAN_STATUS="warning"
              VULN_COUNT=$(jq -r '.Results[] | .Vulnerabilities | length // 0' api_scan.json | awk '{sum+=$1} END {print sum}')
              echo "Found $VULN_COUNT vulnerabilities in API image" >> $GITHUB_STEP_SUMMARY
              echo "API image has vulnerabilities that need to be addressed"
            fi
          else
            echo "### API Image" >> $GITHUB_STEP_SUMMARY
            echo "API image not available for scanning" >> $GITHUB_STEP_SUMMARY
          fi

          # Set output for use in other jobs
          echo "scan_status=$SCAN_STATUS" >> $GITHUB_OUTPUT

  deploy-to-ecs:
    name: 'Deploy to ECS'
    needs: [build-and-push, docker-image-scan]
    runs-on: ubuntu-latest
    # Only run on main branch and not in pull requests, and only if ECR repositories were found
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging') && github.event_name != 'pull_request'
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}

    # Define outputs that can be used by other jobs
    outputs:
      ecr_repos_found: ${{ steps.check-ecr-repos.outputs.ecr_repos_found }}
      ecs_services_found: ${{ steps.get-ecs-services.outputs.ecs_services_found }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Configure AWS credentials
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Check if ECR repositories were found in previous job
      - name: Check ECR Repositories
        id: check-ecr-repos
        run: |
          # Get the value from the previous job's output
          if [[ "${{ needs.build-and-push.outputs.ecr_repos_found }}" != "true" ]]; then
            echo "ECR repositories not found. Skipping deployment."
            echo "ecr_repos_found=false" >> $GITHUB_OUTPUT
          else
            echo "ECR repositories found. Proceeding with deployment."
            echo "ecr_repos_found=true" >> $GITHUB_OUTPUT
          fi

      # Initialize Terraform to get outputs
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      # Initialize Terraform with proper backend configuration
      - name: Terraform Init
        run: |
          # Initialize with environment-specific state file
          terraform init -reconfigure -backend-config="key=environments/${{ env.ENVIRONMENT }}/terraform.tfstate"
          echo "Terraform initialized with environment-specific state file."

      # Get ECS service names from Terraform outputs
      - name: Get ECS Service Names
        id: get-ecs-services
        if: steps.check-ecr-repos.outputs.ecr_repos_found == 'true'
        run: |
          CLUSTER_NAME=$(terraform output -raw ecs_cluster_name || echo "")
          API_SERVICE=$(terraform output -raw api_service_name || echo "")

          # Check if services were found
          if [ -z "$CLUSTER_NAME" ] || [ -z "$API_SERVICE" ]; then
            echo "ECS services not found in Terraform outputs. Trying alternative approach."
            CLUSTER_NAME="${{ secrets.ECS_CLUSTER_NAME }}"
            if [ -z "$CLUSTER_NAME" ]; then
              echo "ECS cluster name not found. Skipping deployment."
              echo "ecs_services_found=false" >> $GITHUB_OUTPUT
              exit 0
            fi
            echo "ecs_services_found=partial" >> $GITHUB_OUTPUT
          else
            echo "ECS services found. Proceeding with deployment."
            echo "ecs_services_found=true" >> $GITHUB_OUTPUT
            echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
            echo "api_service=$API_SERVICE" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      # Install yq for Docker Compose file manipulation
      - name: Install yq
        if: steps.check-ecr-repos.outputs.ecr_repos_found == 'true' && steps.get-ecs-services.outputs.ecs_services_found != 'true'
        run: |
          sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq
          sudo chmod +x /usr/bin/yq
          yq --version

      # Install Docker CLI ECS integration
      - name: Install Docker CLI ECS integration
        if: steps.check-ecr-repos.outputs.ecr_repos_found == 'true' && steps.get-ecs-services.outputs.ecs_services_found != 'true'
        run: |
          echo "Installing Docker CLI ECS integration..."
          # Install the Docker Compose CLI for ECS
          curl -L https://raw.githubusercontent.com/docker/compose-cli/main/scripts/install/install_linux.sh | sh
          # Verify installation
          docker context ls
          echo "Docker CLI ECS integration installed successfully."

      # Prepare ECS-specific Docker Compose file
      - name: Prepare ECS-specific Docker Compose file
        id: prepare-compose
        if: steps.check-ecr-repos.outputs.ecr_repos_found == 'true' && steps.get-ecs-services.outputs.ecs_services_found != 'true'
        env:
          API_IMAGE_URI: ${{ needs.build-and-push.outputs.api_image_uri }}
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
          LOG_GROUP_NAME_ENV: "/ecs/${{ env.COMPOSE_PROJECT_NAME }}"
        run: |
          echo "Preparing docker-compose.ecs.yml from docker-compose.yaml..."
          cp docker-compose.yaml docker-compose.ecs.yml

          # Modify the docker-compose file for ECS deployment
          # This is a simplified version - adjust according to your actual docker-compose structure
          yq -i 'del(.services.postgres)' docker-compose.ecs.yml || true
          yq -i 'del(.services.redis)' docker-compose.ecs.yml || true

          # Only keep the API service as per requirements
          yq -i 'del(.services.frontend)' docker-compose.ecs.yml || true
          yq -i 'del(.services.client)' docker-compose.ecs.yml || true

          # Update API image reference
          if [ ! -z "$API_IMAGE_URI" ]; then
            yq -i '.services.api.image = env(API_IMAGE_URI)' docker-compose.ecs.yml || true
            yq -i 'del(.services.api.build)' docker-compose.ecs.yml || true
          fi

          # Add AWS logging configuration
          for service in $(yq -r '.services | keys | .[]' docker-compose.ecs.yml); do
            yq -i ".services.$service.logging.driver = \"awslogs\"" docker-compose.ecs.yml || true
            yq -i ".services.$service.logging.options.\"awslogs-group\" = env(LOG_GROUP_NAME_ENV)" docker-compose.ecs.yml || true
            yq -i ".services.$service.logging.options.\"awslogs-region\" = env(AWS_REGION_ENV)" docker-compose.ecs.yml || true
            yq -i ".services.$service.logging.options.\"awslogs-stream-prefix\" = \"$service\"" docker-compose.ecs.yml || true
          done

          echo "compose_file_path=docker-compose.ecs.yml" >> $GITHUB_OUTPUT

      # Deploy to ECS using Docker CLI
      - name: Deploy to ECS using Docker CLI
        if: steps.check-ecr-repos.outputs.ecr_repos_found == 'true' && steps.get-ecs-services.outputs.ecs_services_found != 'true'
        env:
          AWS_REGION: ${{ env.AWS_REGION }}
          ECS_CLUSTER_NAME: ${{ secrets.ECS_CLUSTER_NAME }}
        run: |
          echo "Deploying services from ${{ steps.prepare-compose.outputs.compose_file_path }} to ECS cluster: ${ECS_CLUSTER_NAME}"
          docker ecs compose --cluster ${ECS_CLUSTER_NAME} --aws-region ${AWS_REGION} \
            -f ${{ steps.prepare-compose.outputs.compose_file_path }} \
            --project-name ${COMPOSE_PROJECT_NAME} up
          echo "ECS deployment initiated. Monitor progress in AWS Console (ECS & CloudFormation)."
        continue-on-error: true

      # Deploy new versions to ECS services using AWS CLI
      - name: Force New ECS Deployment
        id: deploy
        if: steps.check-ecr-repos.outputs.ecr_repos_found == 'true' && steps.get-ecs-services.outputs.ecs_services_found == 'true'
        run: |
          echo "Starting deployment to ECS..."
          aws ecs update-service --cluster ${{ steps.get-ecs-services.outputs.cluster_name }} --service ${{ steps.get-ecs-services.outputs.api_service }} --force-new-deployment
          echo "Deployment completed successfully!"
        continue-on-error: true

      # Skip deployment message
      - name: Skip Deployment Message
        if: steps.check-ecr-repos.outputs.ecr_repos_found != 'true' || (steps.get-ecs-services.outputs.ecs_services_found != 'true' && steps.get-ecs-services.outputs.ecs_services_found != 'partial')
        run: |
          echo "Skipping deployment to ECS because:"
          if [ "${{ steps.check-ecr-repos.outputs.ecr_repos_found }}" != "true" ]; then
            echo "- ECR repositories were not found"
          fi
          if [ "${{ steps.get-ecs-services.outputs.ecs_services_found }}" != "true" ] && [ "${{ steps.get-ecs-services.outputs.ecs_services_found }}" != "partial" ]; then
            echo "- ECS services were not found"
          fi

      # Send notification on completion
      - name: Notify Deployment Status
        if: always()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "${{ job.status == 'success' && '✅ Deployment Successful' || '❌ Deployment Failed' }} for `${{ github.repository }}`",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "${{ job.status == 'success' && '✅ *Deployment Successful*' || '❌ *Deployment Failed*' }}\nRepository: `${{ github.repository }}`\nBranch: `${{ github.ref_name }}`\nEnvironment: `${{ env.ENVIRONMENT }}`"
                  }
                },
                {
                  "type": "context",
                  "elements": [
                    { "type": "mrkdwn", "text": "Triggered by: `${{ github.actor }}`" },
                    { "type": "mrkdwn", "text": "Workflow Run: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Workflow>" }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        continue-on-error: true

  # Rollback job that runs when deployment fails
  rollback:
    name: 'Rollback Failed Deployment'
    needs: [terraform, deploy-to-ecs]
    runs-on: ubuntu-latest
    if: always() && (needs.terraform.result == 'failure' || needs.deploy-to-ecs.result == 'failure') && needs.terraform.outputs.backup_created == 'true'
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Configure AWS credentials
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Setup Terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      # Download state backup
      - name: Download State Backup
        uses: actions/download-artifact@v4
        with:
          name: terraform-state-backup-${{ github.event.inputs.environment || 'staging' }}
          path: .

      # Initialize Terraform with proper backend configuration
      - name: Terraform Init
        run: |
          # Initialize with environment-specific state file
          terraform init -reconfigure -backend-config="key=environments/${{ env.ENVIRONMENT }}/terraform.tfstate"
          echo "Terraform initialized with environment-specific state file."

      # Restore previous state
      - name: Restore Previous State
        run: |
          echo "Restoring previous state from backup..."
          terraform state push terraform.tfstate
          terraform apply -auto-approve -var="environment=${{ github.event.inputs.environment || 'staging' }}"
          echo "Rollback completed successfully!"

      # Notify rollback status
      - name: Notify Rollback Status
        if: always()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "${{ job.status == 'success' && '✅ Rollback Successful' || '❌ Rollback Failed' }} for `${{ github.repository }}`",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "${{ job.status == 'success' && '✅ *Rollback Completed Successfully*' || '❌ *Rollback Failed*' }}\nRepository: `${{ github.repository }}`\nEnvironment: `${{ github.event.inputs.environment || 'staging' }}`"
                  }
                },
                {
                  "type": "context",
                  "elements": [
                    { "type": "mrkdwn", "text": "Triggered by: `${{ github.actor }}`" },
                    { "type": "mrkdwn", "text": "Workflow Run: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Workflow>" }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        continue-on-error: true

  # Summary job to report overall workflow status
  deployment-summary:
    name: 'Deployment Summary'
    needs: [terraform-destroy, terraform, build-and-push, docker-image-scan, deploy-to-ecs, rollback, vulnerability-scan]
    runs-on: ubuntu-latest
    if: always()
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Set a variable to check if destroy job was run
      - name: Check if destroy job was run
        id: check-destroy
        run: |
          if [[ "${{ needs.terraform-destroy.result }}" != "" ]]; then
            echo "destroy_run=true" >> $GITHUB_OUTPUT
          else
            echo "destroy_run=false" >> $GITHUB_OUTPUT
          fi

      # Configure AWS credentials for Terraform outputs
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
        continue-on-error: true

      # Setup Terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false
        continue-on-error: true

      # Initialize Terraform with proper backend configuration
      - name: Terraform Init
        run: |
          # Initialize with environment-specific state file
          terraform init -reconfigure -backend-config="key=environments/${{ env.ENVIRONMENT }}/terraform.tfstate"
          echo "Terraform initialized with environment-specific state file."
        continue-on-error: true

      - name: Generate Deployment Summary
        id: summary
        run: |
          echo "## Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "| --- | ------ |" >> $GITHUB_STEP_SUMMARY

          # Always include vulnerability scan results if it was run
          if [[ "${{ contains(needs.*.result, needs.vulnerability-scan.result) }}" == "true" ]]; then
            echo "| Vulnerability Scanning | ${{ needs.vulnerability-scan.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          # Check if destroy job was run
          if [[ "${{ steps.check-destroy.outputs.destroy_run }}" == "true" ]]; then
            echo "| Infrastructure Destruction | ${{ needs.terraform-destroy.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Infrastructure destruction completed at $(date)" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Infrastructure Deployment | ${{ needs.terraform.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Docker Image Build & Push | ${{ needs.build-and-push.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Docker Image Scanning | ${{ needs.docker-image-scan.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| ECS Deployment | ${{ needs.deploy-to-ecs.result }} |" >> $GITHUB_STEP_SUMMARY

            # Check if rollback was performed
            if [[ "${{ contains(needs.*.result, needs.rollback.result) }}" == "true" ]]; then
              echo "| Deployment Rollback | ${{ needs.rollback.result }} |" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Note:** A rollback was performed due to deployment failure in the ${{ github.event.inputs.environment || 'staging' }} environment." >> $GITHUB_STEP_SUMMARY
            fi

            # Get the load balancer DNS from Terraform outputs
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Application Access" >> $GITHUB_STEP_SUMMARY

            # Get the load balancer DNS
            LB_DNS=$(terraform output -raw load_balancer_dns 2>/dev/null || echo "Not available")

            if [ "$LB_DNS" != "Not available" ]; then
              echo "Application URL: https://$LB_DNS" >> $GITHUB_STEP_SUMMARY

              # Generate Load Balancer DNS Report
              echo "## Load Balancer DNS Report" >> $GITHUB_STEP_SUMMARY
              echo "### Load Balancer DNS" >> $GITHUB_STEP_SUMMARY
              echo "Found 1 Load Balancer DNS: $LB_DNS" >> $GITHUB_STEP_SUMMARY
            else
              echo "Load balancer DNS not available" >> $GITHUB_STEP_SUMMARY
            fi

            # Generate Infrastructure Report
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Infrastructure Report" >> $GITHUB_STEP_SUMMARY
            echo "The following outputs are available from the deployed infrastructure:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            terraform output 2>/dev/null || echo "No terraform outputs available" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Deployment for environment: **${{ env.ENVIRONMENT }}**" >> $GITHUB_STEP_SUMMARY
            echo "Completed at $(date)" >> $GITHUB_STEP_SUMMARY
          fi

      # Create and save infrastructure report as an artifact
      - name: Save Infrastructure Report as Artifact
        if: always() && steps.check-destroy.outputs.destroy_run != 'true'
        run: |
          mkdir -p infrastructure-report
          echo "# Infrastructure Deployment Report" > infrastructure-report/terraform-outputs.md
          echo "Environment: ${{ env.ENVIRONMENT }}" >> infrastructure-report/terraform-outputs.md
          echo "Deployment Date: $(date)" >> infrastructure-report/terraform-outputs.md
          echo "" >> infrastructure-report/terraform-outputs.md
          echo "## Terraform Outputs" >> infrastructure-report/terraform-outputs.md
          echo '```' >> infrastructure-report/terraform-outputs.md
          terraform output 2>/dev/null || echo "No terraform outputs available" >> infrastructure-report/terraform-outputs.md
          echo '```' >> infrastructure-report/terraform-outputs.md

          # Also save as JSON for potential programmatic use
          terraform output -json 2>/dev/null > infrastructure-report/terraform-outputs.json || echo "{}" > infrastructure-report/terraform-outputs.json

      # Upload the infrastructure report as an artifact
      - name: Upload Infrastructure Report
        if: always() && steps.check-destroy.outputs.destroy_run != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: infrastructure-report-${{ env.ENVIRONMENT }}
          path: infrastructure-report/
          retention-days: 30
          if-no-files-found: warn

      # Send notification for deployment
      - name: Send Deployment Summary Notification
        if: always() && steps.check-destroy.outputs.destroy_run == 'false'
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "Deployment Summary for `${{ github.repository }}` - ${{ env.ENVIRONMENT }} Environment",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Deployment Summary - ${{ env.ENVIRONMENT }} Environment*\n\n*Environment*: ${{ env.ENVIRONMENT }}\n${{ contains(needs.*.result, needs.vulnerability-scan.result) && format('*Vulnerability Scanning*: {0}', needs.vulnerability-scan.result) || '' }}\n*Infrastructure*: ${{ needs.terraform.result }}\n*Docker Images*: ${{ needs.build-and-push.result }}\n*Docker Image Scanning*: ${{ needs.docker-image-scan.result }}\n*ECS Deployment*: ${{ needs.deploy-to-ecs.result }}\n${{ contains(needs.*.result, needs.rollback.result) && format('*Rollback*: {0}', needs.rollback.result) || '' }}\n\n${{ contains(needs.*.result, needs.rollback.result) && '⚠️ *A rollback was performed due to deployment failure*' || '' }}\n${{ contains(needs.*.result, needs.vulnerability-scan.result) && needs.vulnerability-scan.result != 'success' && '⚠️ *Vulnerabilities were found during dependency scanning*' || '' }}\n${{ contains(needs.*.result, needs.docker-image-scan.result) && needs.docker-image-scan.outputs.scan_status == 'warning' && '⚠️ *Vulnerabilities were found in Docker images, but deployment continued*' || '' }}"
                  }
                },
                {
                  "type": "context",
                  "elements": [
                    { "type": "mrkdwn", "text": "Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" },
                    { "type": "mrkdwn", "text": "Completed at $(date)" }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        continue-on-error: true

      # Send notification for destruction
      - name: Send Destruction Summary Notification
        if: always() && steps.check-destroy.outputs.destroy_run == 'true'
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "Infrastructure Destruction Summary for `${{ github.repository }}` - ${{ env.ENVIRONMENT }} Environment",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Infrastructure Destruction Summary - ${{ env.ENVIRONMENT }} Environment*\n\n*Environment*: ${{ env.ENVIRONMENT }}\n*Infrastructure Destruction*: ${{ needs.terraform-destroy.result }}"
                  }
                },
                {
                  "type": "context",
                  "elements": [
                    { "type": "mrkdwn", "text": "Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" },
                    { "type": "mrkdwn", "text": "Completed at $(date)" }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        continue-on-error: true