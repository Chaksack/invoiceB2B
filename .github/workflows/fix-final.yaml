name: InvoiceB2B CI/CD Pipeline

# Workflow for deploying infrastructure using Terraform and deploying applications to ECS
on:
  push:
    branches: [ main, staging ]
  pull_request:
    branches: [ main, staging ]
    types: [opened, synchronize, reopened]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        type: choice
        options:
          - dev
          - staging
          - prod
        default: 'staging'
      destroy_infrastructure:
        description: 'Destroy infrastructure instead of deploying'
        required: false
        type: boolean
        default: false
      confirmation:
        description: 'Type "destroy" to confirm destruction of infrastructure (only needed when destroy_infrastructure is true)'
        required: false
        type: string

permissions:
  contents: read
  pull-requests: write # For SonarQube comments or PR labels
  issues: write     # For manual approval issues

env:
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  TERRAFORM_VERSION: 1.7.0
  ENVIRONMENT: ${{ github.event.inputs.environment || 'staging' }}

jobs:
  setup-terraform:
    name: 'Setup Terraform'
    runs-on: ubuntu-latest
    outputs:
      terraform_initialized: ${{ steps.terraform_init_and_bootstrap.outputs.initialized }}
      project_name: ${{ steps.extract_project_name.outputs.project_name }}
      # Output the effective project name used for backend resources for consistency if needed by other jobs directly
      # though it's better if other jobs also calculate it or receive it via backend config.
      effective_project_name_for_backend: ${{ steps.determine_effective_project_name.outputs.effective_name }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Extract default project_name from variables.tf
        id: extract_project_name # This gets the general project name
        run: |
          project_name_default=$(grep -A 3 'variable "project_name"' variables.tf | grep 'default' | sed -E 's/.*"([^"]+)".*/\1/')
          if [ -z "$project_name_default" ]; then
            echo "Error: Could not extract default project_name from variables.tf"
            exit 1
          fi
          echo "project_name=$project_name_default" >> $GITHUB_OUTPUT # Output the default project name
          echo "Default project_name from variables.tf: $project_name_default"

      - name: Determine Effective Project Name for Backend
        id: determine_effective_project_name
        env:
          DEFAULT_PROJECT_NAME: ${{ steps.extract_project_name.outputs.project_name }}
          TF_ENVIRONMENT: ${{ env.ENVIRONMENT }}
        run: |
          EFFECTIVE_NAME=""
          if [ "$TF_ENVIRONMENT" = "dev" ]; then
            EFFECTIVE_NAME="invoicedev" # Specific prefix for dev
            echo "Using dev-specific effective project name for backend: $EFFECTIVE_NAME"
          else
            EFFECTIVE_NAME="$DEFAULT_PROJECT_NAME" # Standard project name
            echo "Using standard effective project name for backend: $EFFECTIVE_NAME"
          fi
          echo "effective_name=$EFFECTIVE_NAME" >> $GITHUB_OUTPUT
          # These are for constructing S3/DynamoDB names based on the effective project name
          echo "Effective S3 bucket will be: $EFFECTIVE_NAME-terraform-state"
          echo "Effective DynamoDB table will be: $EFFECTIVE_NAME-terraform-locks"

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Cache Terraform
        uses: actions/cache@v4
        with:
          path: ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-terraform-${{ hashFiles('**/.terraform.lock.hcl') }}
          restore-keys: |
            ${{ runner.os }}-terraform-

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Init and Bootstrap Backend
        id: terraform_init_and_bootstrap # Renamed step id for clarity
        env:
          # Use the effective project name determined in the previous step for backend resource names
          EFFECTIVE_PROJECT_NAME: ${{ steps.determine_effective_project_name.outputs.effective_name }}
          TF_ENVIRONMENT: ${{ env.ENVIRONMENT }}
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
          AWS_ACCOUNT_ID_ENV: ${{ secrets.AWS_ACCOUNT_ID }}
        run: |
          # Construct backend resource names using the EFFECTIVE_PROJECT_NAME
          S3_BUCKET_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-state"
          DYNAMODB_TABLE_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-locks"
          TF_STATE_KEY="environments/${TF_ENVIRONMENT}/terraform.tfstate"

          echo "Target S3 Bucket: $S3_BUCKET_NAME"
          echo "Target S3 Key: $TF_STATE_KEY"
          echo "Target DynamoDB Table: $DYNAMODB_TABLE_NAME"
          echo "Target AWS Region: $AWS_REGION_ENV"

          create_s3=false
          create_dynamodb=false
          bootstrapped_locally=false

          # Check S3 bucket
          if ! aws s3api head-bucket --bucket "$S3_BUCKET_NAME" ${AWS_ACCOUNT_ID_ENV:+--expected-bucket-owner "$AWS_ACCOUNT_ID_ENV"} >/dev/null 2>&1; then
            echo "S3 bucket $S3_BUCKET_NAME does not exist or is not accessible. Will attempt to create it via Terraform."
            create_s3=true
          else
            echo "S3 bucket $S3_BUCKET_NAME already exists."
          fi

          # Check DynamoDB table
          if ! aws dynamodb describe-table --table-name "$DYNAMODB_TABLE_NAME" --region "$AWS_REGION_ENV" >/dev/null 2>&1; then
            echo "DynamoDB table $DYNAMODB_TABLE_NAME does not exist in region $AWS_REGION_ENV. Will attempt to create it via Terraform."
            create_dynamodb=true
          else
            echo "DynamoDB table $DYNAMODB_TABLE_NAME already exists in region $AWS_REGION_ENV."
          fi

          if [ "$create_s3" = true ] || [ "$create_dynamodb" = true ]; then
            echo "Attempting to create backend resources using Terraform (bootstrap.tf)..."
            # Ensure bootstrap.tf uses 'variable "project_name"' which will be set to EFFECTIVE_PROJECT_NAME
            # Remove any dynamically generated backend.tf to ensure local init for bootstrap
            rm -f backend.tf backend.generated.tf

            echo "Initializing Terraform locally for bootstrap operation..."
            terraform init -input=false -backend=false
            echo "Bootstrap local init complete."

            apply_targets=""
            if [ "$create_s3" = true ]; then
              apply_targets="$apply_targets -target=aws_s3_bucket.terraform_state -target=aws_s3_bucket_versioning.terraform_state -target=aws_s3_bucket_server_side_encryption_configuration.terraform_state -target=aws_s3_bucket_public_access_block.terraform_state"
            fi
            if [ "$create_dynamodb" = true ]; then
              apply_targets="$apply_targets -target=aws_dynamodb_table.terraform_locks"
            fi

            if [ -n "$apply_targets" ]; then
              echo "Applying bootstrap targets: $apply_targets"
              # Pass EFFECTIVE_PROJECT_NAME as the 'project_name' variable to bootstrap.tf
              terraform apply -auto-approve -input=false -var="project_name=${EFFECTIVE_PROJECT_NAME}" $apply_targets
              echo "Bootstrap apply of backend resources complete."
              bootstrapped_locally=true
            else
              echo "No new backend resources to create via terraform apply for bootstrap."
            fi
          fi

          echo "Generating backend.tf for S3 backend structure (terraform { backend \"s3\" {} })..."
          if [ ! -f ./generate-backend.sh ]; then echo "Error: ./generate-backend.sh script not found!"; exit 1; fi
          chmod +x ./generate-backend.sh
          ./generate-backend.sh "$TF_ENVIRONMENT" # Script should create the basic backend "s3" {} block

          INIT_ARGS="-input=false -reconfigure"
          if [ "$bootstrapped_locally" = true ] && [ -f ".terraform/terraform.tfstate" ]; then
            echo "Local bootstrap performed and local state found. Adding -force-copy to init for state migration."
            INIT_ARGS="$INIT_ARGS -force-copy"
          fi

          echo "Initializing Terraform with S3 backend configuration..."
          terraform init $INIT_ARGS \
            -backend-config="bucket=$S3_BUCKET_NAME" \
            -backend-config="key=$TF_STATE_KEY" \
            -backend-config="region=$AWS_REGION_ENV" \
            -backend-config="dynamodb_table=$DYNAMODB_TABLE_NAME" \
            -backend-config="encrypt=true"

          echo "Terraform S3 backend initialization complete for environment $TF_ENVIRONMENT."
          echo "initialized=true" >> $GITHUB_OUTPUT

  validate-destroy-confirmation:
    name: 'Validate Destroy Confirmation'
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.destroy_infrastructure == 'true'
    steps:
      - name: Check confirmation
        if: ${{ github.event.inputs.confirmation != 'destroy' }}
        run: |
          echo "Error: You must type 'destroy' to confirm infrastructure destruction."
          exit 1

  vulnerability-scan:
    name: 'Scan for Vulnerabilities'
    runs-on: ubuntu-latest
    if: github.event_name != 'workflow_dispatch' || github.event.inputs.destroy_infrastructure != 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '20'

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.22'

      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ runner.os }}-go-

      - name: Scan Frontend Dependencies
        run: |
          if [ -d "frontend" ]; then
            cd frontend
            echo "Running npm audit on frontend dependencies..."
            npm audit --production --audit-level=high || echo "Frontend has vulnerabilities that need to be addressed"
            cd ..
          else
            echo "Frontend directory not found, skipping npm audit."
          fi

      - name: Scan Client Dependencies
        run: |
          if [ -d "client" ]; then
            cd client
            echo "Running go mod verify for client dependencies..."
            go mod verify || echo "Client has dependency issues that need to be addressed"
            cd ..
          else
            echo "Client directory not found, skipping go mod verify."
          fi

      - name: Setup Terraform for tfsec
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Install tfsec
        run: |
          curl -s https://raw.githubusercontent.com/aquasecurity/tfsec/master/scripts/install_linux.sh | bash

      - name: Scan Terraform Code
        run: |
          echo "Running tfsec on Terraform code..."
          # Ensure tfsec runs from the correct directory if TF files are not in root
          tfsec . --no-color || echo "Terraform has security issues that need to be addressed"

      - name: SonarQube Scan
        uses: sonarsource/sonarqube-scan-action@v2.0.0
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        continue-on-error: true

      - name: Generate Vulnerability Report
        run: |
          echo "## Dependency Vulnerability Scan Results" >> $GITHUB_STEP_SUMMARY
          echo "### Frontend Dependencies" >> $GITHUB_STEP_SUMMARY
          if [ -d "frontend" ]; then
            cd frontend && npm audit --json | jq -r '.advisories | length | "Found \(.) vulnerabilities"' >> $GITHUB_STEP_SUMMARY || echo "Error running npm audit for frontend" >> $GITHUB_STEP_SUMMARY
            cd ..
          else
            echo "Frontend directory not found." >> $GITHUB_STEP_SUMMARY
          fi

          echo "### Client Dependencies (Golang)" >> $GITHUB_STEP_SUMMARY
          if [ -d "client" ]; then
            cd client && go mod verify >> $GITHUB_STEP_SUMMARY 2>&1 || echo "Error verifying Go modules for client" >> $GITHUB_STEP_SUMMARY
            cd ..
          else
            echo "Client directory not found." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "### Terraform Security Issues" >> $GITHUB_STEP_SUMMARY
          tfsec . --no-color --format json | jq -r '.results | length | "Found \(.) security issues"' >> $GITHUB_STEP_SUMMARY || echo "Error running tfsec" >> $GITHUB_STEP_SUMMARY

  terraform-destroy:
    name: 'Destroy Infrastructure'
    runs-on: ubuntu-latest
    needs: [validate-destroy-confirmation, setup-terraform]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.destroy_infrastructure == 'true'
    permissions:
      issues: write
      contents: read
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Init for Destroy
        env:
          # Use the effective project name from setup-terraform job for consistency
          EFFECTIVE_PROJECT_NAME: ${{ needs.setup-terraform.outputs.effective_project_name_for_backend }}
          TF_ENVIRONMENT: ${{ env.ENVIRONMENT }}
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
        run: |
          S3_BUCKET_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-state"
          DYNAMODB_TABLE_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-locks"
          TF_STATE_KEY="environments/${TF_ENVIRONMENT}/terraform.tfstate"

          echo "Generating backend.tf for S3 backend structure (destroy)..."
          if [ ! -f ./generate-backend.sh ]; then echo "Error: ./generate-backend.sh script not found!"; exit 1; fi
          chmod +x ./generate-backend.sh
          ./generate-backend.sh "$TF_ENVIRONMENT"

          terraform init -input=false -reconfigure \
            -backend-config="bucket=$S3_BUCKET_NAME" \
            -backend-config="key=$TF_STATE_KEY" \
            -backend-config="region=$AWS_REGION_ENV" \
            -backend-config="dynamodb_table=$DYNAMODB_TABLE_NAME" \
            -backend-config="encrypt=true"
          echo "Terraform initialized for destroy operation."

      - name: Terraform Plan Destroy
        env:
          # Pass the correct project name variable that your main TF config expects,
          # which might be the default one, not the effective one used for backend names.
          PROJECT_NAME_VAR: ${{ needs.setup-terraform.outputs.project_name }}
          TF_ENVIRONMENT_VAR: ${{ env.ENVIRONMENT }}
        run: |
          # The main S3 bucket resource defined in bootstrap.tf should have prevent_destroy = true
          # So, a general 'terraform plan -destroy' should respect that.
          # No need to manually exclude targets if prevent_destroy is set on the bootstrap resources.
          echo "Note: The S3 state bucket and DynamoDB lock table (if defined in bootstrap.tf and part of this state) should have 'prevent_destroy = true' in their definitions."
          terraform plan -destroy \
            -var="project_name=${PROJECT_NAME_VAR}" \
            -var="environment=${TF_ENVIRONMENT_VAR}" \
            -out=tfdestroyplan

      - name: Manual Approval for Destruction
        uses: trstringer/manual-approval@v1 # Consider v1.9.0 or later
        with:
          secret: ${{ github.TOKEN }}
          approvers: ${{ github.actor }}
          minimum-approvals: 1
          issue-title: "Approve Infrastructure Destruction for ${{ env.ENVIRONMENT }}"
          issue-body: "Please approve the destruction of the infrastructure for environment ${{ env.ENVIRONMENT }}. Review plan above."
          exclude-workflow-initiator-as-approver: false # Set to true if initiator should not self-approve
          timeout-minutes: 10

      - name: Terraform Destroy
        run: |
          echo "Applying destroy plan..."
          terraform apply -auto-approve tfdestroyplan
          # The complex loop for targeted destroy is generally risky and might indicate underlying issues.
          # It's often better to ensure dependencies are handled correctly or resources are properly tagged/managed.
          # If a destroy fails, it's usually due to dependencies or permissions.

      - name: Notify Destruction Status
        if: always()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "${{ job.status == 'success' && '✅ Terraform Infrastructure Destroyed Successfully' || '❌ Terraform Destroy Failed' }} for `${{ github.repository }}` on `${{ env.ENVIRONMENT }}`",
              "blocks": [
                { "type": "section", "text": { "type": "mrkdwn", "text": "${{ job.status == 'success' && '✅ *Terraform Infrastructure Destroyed Successfully*' || '❌ *Terraform Destroy Failed*' }}\nRepository: `${{ github.repository }}`\nEnvironment: `${{ env.ENVIRONMENT }}`" }},
                { "type": "context", "elements": [ { "type": "mrkdwn", "text": "Triggered by: `${{ github.actor }}`" }, { "type": "mrkdwn", "text": "Workflow Run: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Workflow>" } ]}
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  terraform:
    name: 'Deploy Infrastructure'
    needs: [vulnerability-scan, setup-terraform]
    runs-on: ubuntu-latest
    # Ensure setup-terraform actually completed successfully and initialized
    if: always() && (github.event_name != 'workflow_dispatch' || github.event.inputs.destroy_infrastructure != 'true') && needs.setup-terraform.outputs.terraform_initialized == 'true'
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}
    outputs:
      terraform_success: ${{ steps.terraform_apply.outputs.success }}
      backup_created: ${{ steps.terraform_apply.outputs.backup_created }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Init for Deploy
        env:
          EFFECTIVE_PROJECT_NAME: ${{ needs.setup-terraform.outputs.effective_project_name_for_backend }}
          TF_ENVIRONMENT: ${{ env.ENVIRONMENT }}
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
        run: |
          S3_BUCKET_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-state"
          DYNAMODB_TABLE_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-locks"
          TF_STATE_KEY="environments/${TF_ENVIRONMENT}/terraform.tfstate"

          echo "Generating backend.tf for S3 backend structure (deploy)..."
          if [ ! -f ./generate-backend.sh ]; then echo "Error: ./generate-backend.sh script not found!"; exit 1; fi
          chmod +x ./generate-backend.sh
          ./generate-backend.sh "$TF_ENVIRONMENT"

          terraform init -input=false -reconfigure \
            -backend-config="bucket=$S3_BUCKET_NAME" \
            -backend-config="key=$TF_STATE_KEY" \
            -backend-config="region=$AWS_REGION_ENV" \
            -backend-config="dynamodb_table=$DYNAMODB_TABLE_NAME" \
            -backend-config="encrypt=true"
          echo "Terraform initialized for deploy operation."

      - name: Terraform Format
        run: terraform fmt -check -recursive
        continue-on-error: true

      - name: Terraform Validate
        run: terraform validate

      - name: Terraform Plan
        env:
          # Pass the correct project name variable that your main TF config expects
          PROJECT_NAME_VAR: ${{ needs.setup-terraform.outputs.project_name }}
          TF_ENVIRONMENT_VAR: ${{ env.ENVIRONMENT }}
        run: terraform plan -var="project_name=${PROJECT_NAME_VAR}" -var="environment=${TF_ENVIRONMENT_VAR}" -out=tfplan

      - name: Manual Approval for Production
        if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request' && env.ENVIRONMENT == 'prod'
        uses: trstringer/manual-approval@v1
        with:
          secret: ${{ github.TOKEN }}
          approvers: ${{ github.actor }}
          minimum-approvals: 1
          issue-title: "Approve Production Deployment to ${{ env.ENVIRONMENT }}"
          issue-body: "Please approve the deployment to the production environment by adding a comment with 'approve'."
          exclude-workflow-initiator-as-approver: false
          timeout-minutes: 60

      - name: Terraform Apply
        id: terraform_apply
        if: ( (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging') && github.event_name == 'push' ) || ( github.event_name == 'workflow_dispatch' && github.event.inputs.destroy_infrastructure != 'true' )
        env:
          # Pass the correct project name variable that your main TF config expects
          PROJECT_NAME_VAR: ${{ needs.setup-terraform.outputs.project_name }}
          TF_ENVIRONMENT_VAR: ${{ env.ENVIRONMENT }}
        run: |
          # Apply with the variables that tfplan used.
          # The plan file 'tfplan' already captured these variables.
          if terraform apply -auto-approve -input=false tfplan; then
            echo "Terraform apply succeeded"
            echo "success=true" >> $GITHUB_OUTPUT
            terraform state pull > terraform.tfstate.backup
            echo "backup_created=true" >> $GITHUB_OUTPUT
          else
            echo "Terraform apply failed"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Upload State Backup
        if: steps.terraform_apply.outputs.backup_created == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: terraform-state-backup-${{ env.ENVIRONMENT }}
          path: terraform.tfstate.backup
          retention-days: 7
          if-no-files-found: warn

  build-and-push:
    name: 'Build and Push Docker Images'
    needs: [terraform, setup-terraform] # Added setup-terraform for project_name
    runs-on: ubuntu-latest
    if: (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging') && github.event_name != 'pull_request' && needs.terraform.outputs.terraform_success == 'true'
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}
    outputs:
      ecr_repos_found: ${{ steps.get-ecr-urls.outputs.ecr_repos_found }}
      api_image_uri: ${{ steps.build-image-api.outputs.image }}
      image_tag: ${{ steps.build-image-api.outputs.digest }} # Using digest for a unique tag

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Init for ECR URLs
        env:
          EFFECTIVE_PROJECT_NAME: ${{ needs.setup-terraform.outputs.effective_project_name_for_backend }}
          TF_ENVIRONMENT: ${{ env.ENVIRONMENT }}
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
        run: |
          S3_BUCKET_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-state"
          DYNAMODB_TABLE_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-locks"
          TF_STATE_KEY="environments/${TF_ENVIRONMENT}/terraform.tfstate"

          echo "Generating backend.tf for S3 backend structure (build-and-push)..."
          if [ ! -f ./generate-backend.sh ]; then echo "Error: ./generate-backend.sh script not found!"; exit 1; fi
          chmod +x ./generate-backend.sh
          ./generate-backend.sh "$TF_ENVIRONMENT"

          terraform init -input=false -reconfigure \
            -backend-config="bucket=$S3_BUCKET_NAME" \
            -backend-config="key=$TF_STATE_KEY" \
            -backend-config="region=$AWS_REGION_ENV" \
            -backend-config="dynamodb_table=$DYNAMODB_TABLE_NAME" \
            -backend-config="encrypt=true"

      - name: Get ECR Repository URLs
        id: get-ecr-urls
        run: |
          API_REPO_URL=$(terraform output -raw api_repository_url || echo "")
          if [ -z "$API_REPO_URL" ]; then
            echo "ECR repository URL (api_repository_url) not found in Terraform outputs."
            echo "ecr_repos_found=false" >> $GITHUB_OUTPUT
          else
            echo "API ECR Repository URL: $API_REPO_URL"
            echo "ecr_repos_found=true" >> $GITHUB_OUTPUT
            # ECR_REPOSITORY_API secret should contain just the repo name, e.g., "my-api-repo"
            # The full URI is constructed using steps.login-ecr.outputs.registry
          fi

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      - name: Build and push API image to Amazon ECR
        id: build-image-api
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.login-ecr.outputs.registry }}/${{ secrets.ECR_REPOSITORY_API }}:${{ github.sha }}
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max

      - name: Move cache
        run: |
          rm -rf /tmp/.buildx-cache || true
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache || true

  docker-image-scan:
    name: 'Scan Docker Images for Vulnerabilities'
    needs: build-and-push
    runs-on: ubuntu-latest
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging') && github.event_name != 'pull_request'
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}
    outputs:
      scan_status: ${{ steps.check-vulnerabilities.outputs.scan_status }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      - name: Configure AWS Credentials
        if: needs.build-and-push.outputs.ecr_repos_found == 'true' || needs.build-and-push.outputs.api_image_uri != '' # Check if image was pushed
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - name: Set up Docker
        uses: docker/setup-buildx-action@v3
      - name: Install Trivy
        run: curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin v0.47.0 # Use a specific version
      - name: Prepare Images for Scanning
        id: prepare-images
        run: |
          API_IMAGE_TO_SCAN=""
          # Use the image URI from the build-and-push job's output
          BUILT_IMAGE_URI="${{ needs.build-and-push.outputs.api_image_uri }}"

          if [[ -n "$BUILT_IMAGE_URI" ]]; then
            API_IMAGE_TO_SCAN="$BUILT_IMAGE_URI"
            echo "Using ECR image for scanning: $API_IMAGE_TO_SCAN"
            # Ensure ECR login if pulling from ECR (already logged in if pushed from same runner, but good for explicitness)
            aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $(echo $API_IMAGE_TO_SCAN | cut -d/ -f1)
            docker pull $API_IMAGE_TO_SCAN || echo "Warning: Failed to pull API image $API_IMAGE_TO_SCAN. Scanning might use local cache if available."
          else
            echo "API image URI from build-and-push is empty. Cannot scan ECR image."
            # Optionally, you could try to build locally again as a fallback, but that's less reliable here.
          fi
          echo "api_image_to_scan=$API_IMAGE_TO_SCAN" >> $GITHUB_OUTPUT
      - name: Scan Images for Vulnerabilities
        id: check-vulnerabilities
        run: |
          API_IMAGE="${{ steps.prepare-images.outputs.api_image_to_scan }}"
          SCAN_STATUS="success"

          echo "## Docker Image Vulnerability Scan Results" >> $GITHUB_STEP_SUMMARY
          echo "### API Image" >> $GITHUB_STEP_SUMMARY

          if [ -n "$API_IMAGE" ]; then
            echo "Scanning API Docker image: $API_IMAGE"
            if trivy image --format json --severity HIGH,CRITICAL --exit-code 1 "$API_IMAGE" > api_scan.json; then
              echo "No HIGH or CRITICAL vulnerabilities found in API image." >> $GITHUB_STEP_SUMMARY
            else
              SCAN_STATUS="warning"
              VULN_COUNT=$(jq -r 'try(.Results[].Vulnerabilities | length // 0) // 0' api_scan.json | awk '{sum+=$1} END {print sum}')
              echo "Found $VULN_COUNT HIGH/CRITICAL vulnerabilities in API image." >> $GITHUB_STEP_SUMMARY
              echo "API image has vulnerabilities. Scan report: api_scan.json. Uploading as artifact..."
              # Consider uploading api_scan.json as an artifact here
            fi
          else
            echo "API image not available for scanning." >> $GITHUB_STEP_SUMMARY
            SCAN_STATUS="skipped"
          fi
          echo "scan_status=$SCAN_STATUS" >> $GITHUB_OUTPUT
      - name: Upload Trivy Scan Report
        if: steps.check-vulnerabilities.outputs.scan_status == 'warning' && steps.prepare-images.outputs.api_image_to_scan != ''
        uses: actions/upload-artifact@v4
        with:
          name: trivy-api-scan-report-${{ env.ENVIRONMENT }}
          path: api_scan.json
          retention-days: 7

  deploy-to-ecs:
    name: 'Deploy to ECS'
    needs: [build-and-push, docker-image-scan, setup-terraform]
    runs-on: ubuntu-latest
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging') && github.event_name != 'pull_request' && needs.build-and-push.outputs.api_image_uri != '' # Ensure image was built
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}
    outputs:
      ecs_deployment_status: ${{ steps.deploy-cli.outputs.status || steps.deploy-docker-cli.outputs.status }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Init for ECS Details
        env:
          EFFECTIVE_PROJECT_NAME: ${{ needs.setup-terraform.outputs.effective_project_name_for_backend }}
          TF_ENVIRONMENT: ${{ env.ENVIRONMENT }}
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
        run: |
          S3_BUCKET_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-state"
          DYNAMODB_TABLE_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-locks"
          TF_STATE_KEY="environments/${TF_ENVIRONMENT}/terraform.tfstate"

          echo "Generating backend.tf for S3 backend structure (deploy-to-ecs)..."
          if [ ! -f ./generate-backend.sh ]; then echo "Error: ./generate-backend.sh script not found!"; exit 1; fi
          chmod +x ./generate-backend.sh
          ./generate-backend.sh "$TF_ENVIRONMENT"

          terraform init -input=false -reconfigure \
            -backend-config="bucket=$S3_BUCKET_NAME" \
            -backend-config="key=$TF_STATE_KEY" \
            -backend-config="region=$AWS_REGION_ENV" \
            -backend-config="dynamodb_table=$DYNAMODB_TABLE_NAME" \
            -backend-config="encrypt=true"

      - name: Get ECS Service Names and Decide Deployment Method
        id: get-ecs-details
        run: |
          CLUSTER_NAME_TF=$(terraform output -raw ecs_cluster_name || echo "")
          API_SERVICE_TF=$(terraform output -raw api_service_name || echo "")
          DEPLOY_METHOD="docker-cli" # Default to Docker CLI

          if [ -n "$CLUSTER_NAME_TF" ] && [ -n "$API_SERVICE_TF" ]; then
            echo "ECS Cluster and Service found via Terraform outputs. Using AWS CLI for deployment."
            DEPLOY_METHOD="aws-cli"
            echo "cluster_name=$CLUSTER_NAME_TF" >> $GITHUB_OUTPUT
            echo "api_service=$API_SERVICE_TF" >> $GITHUB_OUTPUT
          else
            echo "ECS Cluster/Service not found via Terraform outputs. Will attempt Docker Compose CLI."
            # Check for secret if TF outputs are missing for Docker CLI method
            if [ -z "${{ secrets.ECS_CLUSTER_NAME }}" ]; then
              echo "Error: Neither Terraform output 'ecs_cluster_name' nor secret 'ECS_CLUSTER_NAME' is available for Docker CLI deployment. Cannot proceed."
              # Set DEPLOY_METHOD to skip to prevent further steps from running if this is critical
              # For now, let it try and potentially fail in the Docker CLI step or be skipped by its condition.
            fi
          fi
          echo "deploy_method=$DEPLOY_METHOD" >> $GITHUB_OUTPUT

      - name: Install yq (for Docker CLI method)
        if: steps.get-ecs-details.outputs.deploy_method == 'docker-cli'
        run: |
          sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq && sudo chmod +x /usr/bin/yq && yq --version

      - name: Install Docker CLI ECS integration (for Docker CLI method)
        if: steps.get-ecs-details.outputs.deploy_method == 'docker-cli'
        run: |
          curl -L https://raw.githubusercontent.com/docker/compose-cli/main/scripts/install/install_linux.sh | sh && docker context ls

      - name: Prepare ECS-specific Docker Compose file (for Docker CLI method)
        id: prepare-compose
        if: steps.get-ecs-details.outputs.deploy_method == 'docker-cli'
        env:
          API_IMAGE_URI_ENV: ${{ needs.build-and-push.outputs.api_image_uri }} # Ensure this is the full URI
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
          COMPOSE_PROJECT_NAME_ENV: ${{ secrets.ECS_PROJECT_NAME || needs.setup-terraform.outputs.effective_project_name_for_backend }}
        run: |
          echo "Preparing docker-compose.ecs.yml..."
          cp docker-compose.yaml docker-compose.ecs.yml # Assume docker-compose.yaml is in repo root

          # Modify for API service only, update image, add logging
          yq -i '.services = {"api": .services.api}' docker-compose.ecs.yml # Keep only API
          if [ -n "$API_IMAGE_URI_ENV" ]; then
            yq -i '.services.api.image = strenv(API_IMAGE_URI_ENV)' docker-compose.ecs.yml
            yq -i 'del(.services.api.build)' docker-compose.ecs.yml
          else
            echo "Error: API_IMAGE_URI_ENV is empty. Cannot set image in Docker Compose file."
            exit 1
          fi
          LOG_GROUP="/ecs/${COMPOSE_PROJECT_NAME_ENV}"
          yq -i ".services.api.logging.driver = \"awslogs\"" docker-compose.ecs.yml
          yq -i ".services.api.logging.options.\"awslogs-group\" = \"$LOG_GROUP\"" docker-compose.ecs.yml
          yq -i ".services.api.logging.options.\"awslogs-region\" = strenv(AWS_REGION_ENV)" docker-compose.ecs.yml
          yq -i ".services.api.logging.options.\"awslogs-stream-prefix\" = \"api\"" docker-compose.ecs.yml

          echo "compose_file_path=docker-compose.ecs.yml" >> $GITHUB_OUTPUT
          echo "compose_project_name_cli=$COMPOSE_PROJECT_NAME_ENV" >> $GITHUB_OUTPUT


      - name: Deploy to ECS using Docker CLI
        id: deploy-docker-cli
        if: steps.get-ecs-details.outputs.deploy_method == 'docker-cli' && steps.prepare-compose.outputs.compose_file_path != '' && secrets.ECS_CLUSTER_NAME != ''
        env:
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
          ECS_CLUSTER_NAME_ENV: ${{ secrets.ECS_CLUSTER_NAME }} # Use secret for Docker CLI
          COMPOSE_PROJECT_NAME_CLI_ENV: ${{ steps.prepare-compose.outputs.compose_project_name_cli }}
        run: |
          echo "Deploying API service from ${{ steps.prepare-compose.outputs.compose_file_path }} to ECS cluster: ${ECS_CLUSTER_NAME_ENV}"
          if docker ecs compose --cluster "${ECS_CLUSTER_NAME_ENV}" --aws-region "${AWS_REGION_ENV}" \
            -f "${{ steps.prepare-compose.outputs.compose_file_path }}" \
            --project-name "${COMPOSE_PROJECT_NAME_CLI_ENV}" up; then
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            exit 1
          fi
        continue-on-error: false # Fail job if Docker CLI deploy fails

      - name: Force New ECS Deployment (AWS CLI)
        id: deploy-cli
        if: steps.get-ecs-details.outputs.deploy_method == 'aws-cli'
        run: |
          echo "Forcing new deployment for service ${{ steps.get-ecs-details.outputs.api_service }} in cluster ${{ steps.get-ecs-details.outputs.cluster_name }}..."
          if aws ecs update-service --cluster "${{ steps.get-ecs-details.outputs.cluster_name }}" --service "${{ steps.get-ecs-details.outputs.api_service }}" --force-new-deployment; then
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            exit 1
          fi
        continue-on-error: false # Fail job if AWS CLI deploy fails

      - name: Notify Deployment Status
        if: always()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "${{ (steps.deploy-cli.outputs.status == 'success' || steps.deploy-docker-cli.outputs.status == 'success') && '✅ ECS Deployment Successful' || '❌ ECS Deployment Failed' }} for `${{ github.repository }}` on `${{ env.ENVIRONMENT }}`",
              "blocks": [ /* ... your slack blocks ... */ ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  rollback:
    name: 'Rollback Failed Deployment'
    needs: [terraform, deploy-to-ecs, setup-terraform]
    runs-on: ubuntu-latest
    # Trigger rollback if terraform apply failed OR if ECS deployment (either method) failed
    if: always() && (needs.terraform.outputs.terraform_success == 'false' || (needs.deploy-to-ecs.result != 'success' && needs.deploy-to-ecs.outputs.ecs_deployment_status == 'failure') ) && needs.terraform.outputs.backup_created == 'true'
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
      - name: Download State Backup
        uses: actions/download-artifact@v4
        with:
          name: terraform-state-backup-${{ env.ENVIRONMENT }} # Ensure this matches upload name
          path: . # Download to current directory
      - name: Terraform Init for Rollback
        env:
          EFFECTIVE_PROJECT_NAME: ${{ needs.setup-terraform.outputs.effective_project_name_for_backend }}
          TF_ENVIRONMENT: ${{ env.ENVIRONMENT }}
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
        run: |
          S3_BUCKET_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-state"
          DYNAMODB_TABLE_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-locks"
          TF_STATE_KEY="environments/${TF_ENVIRONMENT}/terraform.tfstate"

          echo "Generating backend.tf for S3 backend structure (rollback)..."
          if [ ! -f ./generate-backend.sh ]; then echo "Error: ./generate-backend.sh script not found!"; exit 1; fi
          chmod +x ./generate-backend.sh
          ./generate-backend.sh "$TF_ENVIRONMENT"

          terraform init -input=false -reconfigure \
            -backend-config="bucket=$S3_BUCKET_NAME" \
            -backend-config="key=$TF_STATE_KEY" \
            -backend-config="region=$AWS_REGION_ENV" \
            -backend-config="dynamodb_table=$DYNAMODB_TABLE_NAME" \
            -backend-config="encrypt=true"
      - name: Restore Previous State
        env:
          PROJECT_NAME_VAR: ${{ needs.setup-terraform.outputs.project_name }} # For -var="project_name=..."
          TF_ENVIRONMENT_VAR: ${{ env.ENVIRONMENT }} # For -var="environment=..."
        run: |
          echo "Restoring previous state from terraform.tfstate.backup..."
          if [ ! -f terraform.tfstate.backup ]; then
            echo "Error: Backup state file terraform.tfstate.backup not found!"
            exit 1
          fi
          terraform state push -force terraform.tfstate.backup # Use -force with caution
          # Re-apply based on the restored state. This might re-provision to the previous state.
          # Ensure your variables are correctly set for this apply.
          terraform apply -auto-approve -input=false \
            -var="project_name=${PROJECT_NAME_VAR}" \
            -var="environment=${TF_ENVIRONMENT_VAR}"
          echo "Rollback: Terraform apply based on restored state completed."
      - name: Notify Rollback Status
        if: always()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "${{ job.status == 'success' && '✅ Rollback Successful' || '❌ Rollback Failed' }} for `${{ github.repository }}` on `${{ env.ENVIRONMENT }}`",
              "blocks": [ /* ... your slack blocks ... */ ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  deployment-summary:
    name: 'Deployment Summary'
    needs: [terraform-destroy, terraform, build-and-push, docker-image-scan, deploy-to-ecs, rollback, vulnerability-scan, setup-terraform]
    runs-on: ubuntu-latest
    if: always()
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      - name: Check if destroy job was run
        id: check-destroy
        run: |
          if [[ "${{ needs.terraform-destroy.result }}" != "" && "${{ needs.terraform-destroy.result }}" != "skipped" ]]; then
            echo "destroy_run=true" >> $GITHUB_OUTPUT
          else
            echo "destroy_run=false" >> $GITHUB_OUTPUT
          fi
      - name: Configure AWS Credentials
        if: steps.check-destroy.outputs.destroy_run == 'false'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
        continue-on-error: true
      - name: Setup Terraform
        if: steps.check-destroy.outputs.destroy_run == 'false'
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
        continue-on-error: true
      - name: Terraform Init for Summary
        if: steps.check-destroy.outputs.destroy_run == 'false'
        env:
          EFFECTIVE_PROJECT_NAME: ${{ needs.setup-terraform.outputs.effective_project_name_for_backend }}
          TF_ENVIRONMENT: ${{ env.ENVIRONMENT }}
          AWS_REGION_ENV: ${{ env.AWS_REGION }}
        run: |
          S3_BUCKET_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-state"
          DYNAMODB_TABLE_NAME="${EFFECTIVE_PROJECT_NAME}-terraform-locks"
          TF_STATE_KEY="environments/${TF_ENVIRONMENT}/terraform.tfstate"

          echo "Generating backend.tf for S3 backend structure (summary)..."
          if [ ! -f ./generate-backend.sh ]; then echo "Error: ./generate-backend.sh script not found!"; exit 1; fi
          chmod +x ./generate-backend.sh
          ./generate-backend.sh "$TF_ENVIRONMENT"

          terraform init -input=false -reconfigure \
            -backend-config="bucket=$S3_BUCKET_NAME" \
            -backend-config="key=$TF_STATE_KEY" \
            -backend-config="region=$AWS_REGION_ENV" \
            -backend-config="dynamodb_table=$DYNAMODB_TABLE_NAME" \
            -backend-config="encrypt=true"
        continue-on-error: true
      - name: Generate Deployment Summary
        id: summary
        env:
          # Pass the correct project name variable that your main TF config expects for terraform output
          PROJECT_NAME_VAR: ${{ needs.setup-terraform.outputs.project_name }}
          TF_ENVIRONMENT_VAR: ${{ env.ENVIRONMENT }}
        run: |
          echo "## Workflow Summary for Environment: ${{ env.ENVIRONMENT }}" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "| --- | ------ |" >> $GITHUB_STEP_SUMMARY

          # Vulnerability Scan
          if [[ "${{ needs.vulnerability-scan.result }}" != "" && "${{ needs.vulnerability-scan.result }}" != "skipped" ]]; then
            echo "| Vulnerability Scanning | ${{ needs.vulnerability-scan.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          # Destroy vs Deploy Path
          if [[ "${{ steps.check-destroy.outputs.destroy_run }}" == "true" ]]; then
            echo "| Infrastructure Destruction | ${{ needs.terraform-destroy.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Infrastructure destruction process completed at $(date)" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Infrastructure Setup (Backend Init) | ${{ needs.setup-terraform.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Infrastructure Deployment (Terraform Apply) | ${{ needs.terraform.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Docker Image Build & Push | ${{ needs.build-and-push.result }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Docker Image Scanning | ${{ needs.docker-image-scan.result }} (Scan Status: ${{ needs.docker-image-scan.outputs.scan_status }}) |" >> $GITHUB_STEP_SUMMARY
            echo "| ECS Deployment | ${{ needs.deploy-to-ecs.result }} (Status: ${{ needs.deploy-to-ecs.outputs.ecs_deployment_status }}) |" >> $GITHUB_STEP_SUMMARY

            if [[ "${{ needs.rollback.result }}" != "" && "${{ needs.rollback.result }}" != "skipped" ]]; then
              echo "| Deployment Rollback | ${{ needs.rollback.result }} |" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Note:** A rollback was attempted/performed." >> $GITHUB_STEP_SUMMARY
            fi

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Application Access" >> $GITHUB_STEP_SUMMARY
            # Ensure terraform output uses the correct variables if needed
            LB_DNS=$(terraform output -raw -var="project_name=${PROJECT_NAME_VAR}" -var="environment=${TF_ENVIRONMENT_VAR}" load_balancer_dns 2>/dev/null || echo "Not available")
            if [ "$LB_DNS" != "Not available" ] && [ "$LB_DNS" != "" ]; then
              echo "Application URL: https://$LB_DNS" >> $GITHUB_STEP_SUMMARY
            else
              echo "Load balancer DNS not available or not deployed." >> $GITHUB_STEP_SUMMARY
            fi

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Infrastructure Report (Outputs)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            terraform output -var="project_name=${PROJECT_NAME_VAR}" -var="environment=${TF_ENVIRONMENT_VAR}" 2>/dev/null || echo "No terraform outputs available or error fetching them." >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Workflow completed at $(date)" >> $GITHUB_STEP_SUMMARY

      - name: Save Infrastructure Report as Artifact
        if: always() && steps.check-destroy.outputs.destroy_run == 'false'
        env:
          PROJECT_NAME_VAR: ${{ needs.setup-terraform.outputs.project_name }}
          TF_ENVIRONMENT_VAR: ${{ env.ENVIRONMENT }}
        run: |
          mkdir -p infrastructure-report
          echo "# Infrastructure Deployment Report" > infrastructure-report/terraform-outputs.md
          echo "Environment: ${{ env.ENVIRONMENT }}" >> infrastructure-report/terraform-outputs.md
          # ... (rest of the report generation)
          echo '```' >> infrastructure-report/terraform-outputs.md
          terraform output -var="project_name=${PROJECT_NAME_VAR}" -var="environment=${TF_ENVIRONMENT_VAR}" 2>/dev/null || echo "No terraform outputs available" >> infrastructure-report/terraform-outputs.md
          echo '```' >> infrastructure-report/terraform-outputs.md
          terraform output -json -var="project_name=${PROJECT_NAME_VAR}" -var="environment=${TF_ENVIRONMENT_VAR}" 2>/dev/null > infrastructure-report/terraform-outputs.json || echo "{}" > infrastructure-report/terraform-outputs.json
      - name: Upload Infrastructure Report
        if: always() && steps.check-destroy.outputs.destroy_run == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: infrastructure-report-${{ env.ENVIRONMENT }}
          path: infrastructure-report/
          retention-days: 30
          if-no-files-found: warn

      - name: Send Deployment Summary Notification
        if: always() && steps.check-destroy.outputs.destroy_run == 'false'
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "Deployment Summary for `${{ github.repository }}` - ${{ env.ENVIRONMENT }} Environment",
              "blocks": [ /* ... your slack blocks, ensure they use correct needs contexts ... */ ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        continue-on-error: true

      - name: Send Destruction Summary Notification
        if: always() && steps.check-destroy.outputs.destroy_run == 'true'
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "Infrastructure Destruction Summary for `${{ github.repository }}` - ${{ env.ENVIRONMENT }} Environment",
              "blocks": [ /* ... your slack blocks ... */ ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        continue-on-error: true

